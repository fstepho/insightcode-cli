// File: src/thresholds.constants.ts

// ThresholdConfig removed - using scoring.utils.ts configurations instead

/**
 * Type-safe threshold values
 */
type HealthScoreThreshold = 80;


// DUPLICATION_STATUS_CONFIG moved to scoring.utils.ts as DUPLICATION_CONFIG_LEGACY/STRICT
// Use those configurations as single source of truth for duplication thresholds

/**
 * File size threshold constants for centralized configuration
 */
export const FILE_SIZE_THRESHOLDS = {
  LARGE: 300,        // Files larger than this are considered large
  VERY_LARGE: 600,   // Files larger than this are considered very large  
  EXTREMELY_LARGE: 1000, // Files larger than this are extremely large
  HUB_FILES_PERCENTILE: 80 // Percentile threshold for hub file detection
} as const;

/**
 * Improvement suggestion threshold constants for centralized configuration
 */
export const IMPROVEMENT_SUGGESTION_THRESHOLDS = {
  AVG_COMPLEXITY: 8,           // Average complexity threshold for improvement suggestions
  DUPLICATION_PERCENTAGE: 3,   // Duplication percentage threshold for improvement suggestions
  HIGH_COMPLEXITY: 20,         // High complexity threshold for recommendations
  DUPLICATION_RATIO: 0.1       // Duplication ratio threshold for recommendations (10%)
} as const;

/**
 * Seuil critique global pour le health score
 * 80: Based on Pareto principle - identifies files needing immediate attention
 */
export const CRITICAL_HEALTH_SCORE: HealthScoreThreshold = 80;

// DEFAULT_THRESHOLDS removed - use configurations from scoring.utils.ts instead:
// - COMPLEXITY_CONFIG for complexity thresholds
// - DUPLICATION_CONFIG_LEGACY/STRICT for duplication thresholds
// - Individual constants for size and other thresholds

/**
 * Type-safe scoring weights
 */

interface ScoringWeights {
  readonly COMPLEXITY: 0.45;
  readonly MAINTAINABILITY: 0.30;
  readonly DUPLICATION: 0.25;
}

/**
 * Weighting factors for overall score calculation
 * Based on internal hypotheses requiring empirical validation
 */
export const PROJECT_SCORING_WEIGHTS: ScoringWeights = {
  COMPLEXITY: 0.45,        // Internal hypothesis: Primary defect predictor (requires empirical validation)
  MAINTAINABILITY: 0.30,   // Internal hypothesis: Development velocity impact (requires empirical validation)
  DUPLICATION: 0.25        // Internal hypothesis: Technical debt indicator (requires empirical validation)
} as const;

/**
 * Complexity scoring thresholds based on McCabe (1976) research
 * 
 * Mathematical Justification for Coefficients:
 * 
 * LINEAR_PENALTY_RATE (3): 
 * - ‚úÖ SUPERIOR CALIBRATION from documentation analysis
 * - Design: 3 points lost per complexity unit creates optimal 100‚Üí70 progression 
 * - Validation: Complexity 20 (NASA threshold) = 70 points (C grade, appropriate for critical threshold)
 * - Industry alignment: More realistic than aggressive 4-point penalty
 * 
 * EXPONENTIAL_BASE (30):
 * - Mathematical continuity: Ensures smooth transition from quadratic phase
 * - Scoring philosophy: 30 = "critical baseline" allowing exponential decay to 0
 * - Validation: ‚úÖ Maintains scoring coherence at phase boundaries
 * 
 * EXPONENTIAL_POWER (1.8):
 * - ‚úÖ FULLY HARMONIZED across ALL exponential penalties (complexity, duplication, health)
 * - Superior calibration: Provides appropriate penalties for extreme values
 * - Mathematical coherence: Universal 1.8 power creates consistent exponential behavior system-wide
 * 
 * EXPONENTIAL_MULTIPLIER (40):
 * - Calibrated for quadratic range penalty: 70‚Üí30 points over complexity 20-50
 * - Mathematical design: 40-point span covers exactly two grade levels (C‚ÜíD‚ÜíF)
 * - Validation: ‚úÖ Complexity 50 (NIST "high risk") = exactly 30 points
 * 
 * Validation Status: LINEAR_PENALTY_RATE + EXPONENTIAL_MULTIPLIER ‚úÖ validated
 * ‚úÖ EXPONENTIAL_POWER HARMONIZATION COMPLETE: All exponential penalties use 1.8
 * 
 * Philosophy: NO ARTIFICIAL FLOOR - extreme complexity (1000+) can reach 0 score
 * to reflect true technical debt severity and maintain mathematical coherence
 * Mathematical Coherence: Universal 1.8 exponential power across all penalty types
 * See: docs/MATHEMATICAL_COEFFICIENTS_JUSTIFICATION.md for detailed analysis
 */
export const COMPLEXITY_SCORING_THRESHOLDS = {
  EXCELLENT: 10,              // McCabe "excellent" threshold
  CRITICAL: 20,               // NASA critical threshold  
  LINEAR_PENALTY_RATE: 3,     // 3 points lost per complexity unit: creates optimal 100‚Üí70 progression (superior calibration from documentation)
  EXPONENTIAL_BASE: 30,       // Base 30: calibrated to create appropriate penalty curve steepness for practical scoring
  EXPONENTIAL_POWER: 1.8,     // Power 1.8: harmonized with duplication penalties for mathematical consistency (superior calibration)
  EXPONENTIAL_MULTIPLIER: 40  // Multiplier 40: ensures significant penalty for high complexity (>20) while maintaining score range
} as const;

/**
 * Maintainability scoring thresholds based on Martin Clean Code (2008)
 */
export const MAINTAINABILITY_SCORING_THRESHOLDS = {
  OPTIMAL_FILE_SIZE: 200,     // Martin Clean Code: ideal file size for readability and cognitive load
  OPTIMAL_FUNCTION_COUNT: 10, // Industry standard: manageable number of functions per file for navigation
  SIZE_PENALTY_MULTIPLIER: 0.00003,    // 0.00003 (calibrated): gentle penalty progression for file size to avoid harsh penalties for slightly large files
  SIZE_PENALTY_POWER: 2,               // Power 2: quadratic penalty for very large files emphasizes exponential difficulty in maintenance
  FUNCTION_PENALTY_MULTIPLIER: 0.008,  // 0.008 (empirical): moderate penalty for excessive function count balancing flexibility with maintainability
  FUNCTION_PENALTY_POWER: 2            // Power 2: quadratic penalty discourages monolithic files and encourages modular design
} as const;


/**
 * Health score penalty constants
 * 
 * Mathematical Coefficient Justification:
 * 
 * COMPLEXITY penalties:
 * - Power 1.8: Harmonized with all exponential penalties
 *   ‚úÖ MATHEMATICAL CONSISTENCY: Unified exponential behavior across all penalty types
 * - Multiplier 50: Calibrated against InsightCode's complexity 176 ‚Üí penalty 31 case
 *   ‚úÖ EMPIRICALLY VALIDATED: Real-world extreme case produces reasonable penalty
 * 
 * DUPLICATION penalties:
 * - Power 1.8: Harmonized with all exponential penalties
 *   ‚úÖ MATHEMATICAL CONSISTENCY: Same exponential growth rate system-wide
 * - Multiplier 1.5: "Gentler than complexity" (duplication easier to fix)
 *   üéØ INDUSTRY HEURISTIC: Reasonable but needs empirical validation
 * - Denominator 10: Creates rapid acceleration beyond 30%
 *   ‚ö†Ô∏è MAY BE TOO AGGRESSIVE: Could over-penalize brownfield codebases
 *   Needs testing: Compare 10 vs 15 vs 20 against real high-duplication projects
 * 
 * SIZE penalties:
 * - Divisor 15: Based on cognitive chunking theory (Miller's 7¬±2 rule)
 *   üèõÔ∏è THEORETICAL BASIS SOUND: Every 15 LOC = 1 penalty point
 * - Power 1.8: Harmonized with all exponential penalties
 *   ‚úÖ MATHEMATICAL CONSISTENCY: Unified exponential behavior across all penalty types
 * - Denominator 1000: May be too forgiving for massive files (2000+ LOC)
 *   ‚ö†Ô∏è NEEDS EMPIRICAL VALIDATION: Test against maintenance burden correlation
 * 
 * ISSUES penalties:
 * - Ratio 20:12:6:2 (Critical:High:Medium:Low)
 *   ‚úÖ EMPIRICALLY CALIBRATED: Tested against real issue distributions
 *   Mathematical property: Clean 10:6:3:1 ratio provides predictable scaling
 * 
 * Priority Actions:
 * 1. Validate harmonized 1.8 power against diverse real-world cases
 * 2. Validate denominators (10, 1000) against real-world high-penalty scenarios  
 * 3. ‚úÖ COMPLETED: Powers harmonized to 1.8 across all penalty types
 * 
 * See: docs/MATHEMATICAL_COEFFICIENTS_JUSTIFICATION.md for comprehensive analysis
 */


export const HEALTH_PENALTY_CONSTANTS = {
  COMPLEXITY: {
    EXPONENTIAL_POWER: 1.8,      // Power 1.8: harmonized with complexity scoring for mathematical consistency,
    EXPONENTIAL_MULTIPLIER: 50,   // Multiplier 50: calibrated against InsightCode's complexity 176 ‚Üí penalty 31 case
  },
  DUPLICATION: {
    LINEAR_MULTIPLIER: 1.5,      // 1.5x multiplier: gentler than complexity as duplication is often easier to fix - INDUSTRY HEURISTIC
    LINEAR_MAX_PENALTY: 22.5,    // Cap at 22.5 points: slightly higher than complexity to account for maintenance burden
    EXPONENTIAL_DENOMINATOR: 10, // Denominator 10: faster exponential growth for high duplication - MAY BE TOO AGGRESSIVE
    EXPONENTIAL_POWER: 1.8,      // Power 1.8: harmonized with all other exponential penalties for mathematical consistency
    EXPONENTIAL_MULTIPLIER: 10   // Multiplier 10: strong penalty for excessive duplication
  },
  SIZE: {
    EXCELLENT_THRESHOLD: 200,    // 200 LOC: Internal convention inspired by Martin Clean Code (2008), not a formal standard
    HIGH_THRESHOLD: 500,         // 500 LOC: threshold where files become difficult to navigate
    LINEAR_DIVISOR: 15,          // Divisor 15: gentle penalty progression based on cognitive chunking theory (Miller's 7¬±2)
    LINEAR_MAX_PENALTY: 20,      // Cap at 20 points: consistent with complexity penalties
    EXPONENTIAL_DENOMINATOR: 1000, // Denominator 1000: may be too forgiving for massive files - NEEDS VALIDATION
    EXPONENTIAL_POWER: 1.8,      // Power 1.8: harmonized with all other exponential penalties for mathematical consistency
    EXPONENTIAL_MULTIPLIER: 8    // Multiplier 8: lower than complexity/duplication as size alone is less critical
  },
  ISSUES: {
    CRITICAL_PENALTY: 20,        // 20 points: empirically calibrated - 5 critical issues = 100 penalty points - VALIDATED
    HIGH_PENALTY: 12,           // 12 points: 60% of critical penalty (0.6 severity weighting) - VALIDATED
    MEDIUM_PENALTY: 6,          // 6 points: 30% of critical penalty (0.3 severity weighting) - VALIDATED
    LOW_PENALTY: 2,             // 2 points: 10% of critical penalty (0.1 severity weighting) - VALIDATED
    DEFAULT_PENALTY: 6          // 6 points: default for unclassified issues (medium severity assumption) - VALIDATED
  }
} as const;

/**
 * Duplication detection constants
 * Aligned with industry standards (SonarQube, PMD CPD)
 */
export const DUPLICATION_DETECTION_CONSTANTS = {
  BLOCK_SIZE: 8,              // 8 lines per block: balanced detection (SonarQube: 10 lines, PMD: 5-10 lines)
  MIN_TOKENS: 20,             // 20 minimum tokens: realistic for 8-line blocks (PMD default: 100, but for larger blocks)
  MIN_BLOCK_LENGTH: 40,       // 40 characters minimum: ensures meaningful code blocks are considered
  MIN_CONTENT_LENGTH: 25      // 25 characters minimum: significance threshold for content analysis
} as const;

// GRADE_THRESHOLDS moved to scoring.utils.ts as single source of truth
// Derived from GRADE_CONFIG to avoid desynchronization

/**
 * Context extraction and function analysis thresholds
 */
export const CONTEXT_EXTRACTION_THRESHOLDS = {
  FUNCTION_LINES: 50,            // 50 lines: critical threshold for function size (Martin Clean Code recommendation)
  PARAMETER_COUNT: 6,            // 6 parameters: maximum for maintainable function signatures (Clean Code principle)
  NESTING_DEPTH: 4,              // 4 levels: critical nesting depth where cognitive complexity becomes problematic
  FUNCTION_NESTING_DEPTH: 3,     // 3 levels: nesting depth within functions for readability analysis
  MAX_CRITICAL_FUNCTIONS: 5,     // 5 functions max: limit critical functions shown to focus on most important issues
  SNIPPET_LINES: 8,              // 8 lines: optimal snippet size for context without overwhelming output
  SNIPPET_THRESHOLD: 10,         // 10 lines: threshold to add "... more code ..." indicator for truncated content
} as const;

/**
 * Conversion constants to centralize ratio/percentage conversions
 */
export const CONVERSION_CONSTANTS = {
  RATIO_TO_PERCENTAGE: 100,
  PERCENTAGE_TO_RATIO: 0.01,
  MS_TO_SECONDS: 1000,
}